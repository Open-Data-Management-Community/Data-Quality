Great Expectations（简称 GX） 是一个 开源的数据质量管理和验证工具，用于帮助企业和数据团队 自动化检测、监控和报告数据质量。它允许用户为数据定义 数据期望（Expectations），并提供直观的报告和验证机制，以确保数据符合业务规则和技术要求。
1. 核心概念
1.1 Expectation（数据期望）
Expectation 是 数据质量规则，用于定义数据应该满足的条件。例如：
	•	expect_column_values_to_be_between("age", 18, 99)（年龄应在 18-99 之间）
	•	expect_column_values_to_match_regex("email", r"[^@]+@[^@]+\.[^@]+")（邮箱格式必须正确）
GX 提供了 50+ 预定义的 Expectation，并允许用户自定义规则。
1.2 Batch（数据批次）
	•	Batch 是 数据的最小验证单位，通常对应一个文件、一张表或一个时间段的数据快照。
	•	用户可以基于 Batch 进行数据质量检查，并生成验证报告。
1.3 Data Context（数据上下文）
	•	GX 使用 Data Context 来管理所有数据质量相关的配置、期望集（Expectation Suite）、数据源等。
	•	Data Context 既可以本地存储（文件系统），也可以集成到云端或数据库中。
1.4 Checkpoint（检查点）
	•	Checkpoint 是 GX 执行数据验证的机制，它定义了：
	•	要验证的数据源
	•	要使用的期望规则（Expectation Suite）
	•	验证结果的存储位置（如 Data Docs、数据库）
	•	触发的后续操作（如失败通知）
1.5 Data Docs（数据质量报告）
	•	自动生成 HTML 格式的数据质量报告
	•	可用于团队协作，方便数据分析师、工程师和业务人员查看数据质量状况

2. 主要功能
✅ 数据验证：支持结构化数据（SQL、CSV、Parquet、Pandas DataFrame）
✅ 可视化数据质量报告：生成 Data Docs，便于数据分析与协作
✅ 自动化数据测试：在 ETL 管道、数据湖或数据库中执行自动数据质量检查
✅ 数据版本控制：可跟踪不同时间点的数据质量变化
✅ 与大数据和云计算兼容：支持 Databricks, Snowflake, AWS, Azure, Google Cloud
✅ 开源 + 可扩展：可以自定义数据质量规则并与现有工具集成

同一套 Expectation Suite 在多个数据源（source）上运行，可以使用 同一个 Expectation Suite 名称结合不同的BatchRequest。可以在 Checkpoint 或 Validation Operator 里一次性执行多个数据源的验证。
方法1:	创建 Checkpoint 配置 great_expectations/checkpoints/multi_source_checkpoint.yml
name: multi_source_checkpoint
config_version: 1.0
class_name: Checkpoint
validations:
  - batch_request:
      datasource_name: postgres_datasource
      data_connector_name: default_inferred_data_connector_name
      data_asset_name: customers_table
    expectation_suite_name: data_quality_suite
  - batch_request:
      datasource_name: snowflake_datasource
      data_connector_name: default_inferred_data_connector_name
      data_asset_name: transactions_table
    expectation_suite_name: data_quality_suite
  - batch_request:
      datasource_name: s3_datasource
      data_connector_name: default_inferred_data_connector_name
      data_asset_name: logs_parquet
    expectation_suite_name: data_quality_suite
	2.	在 Python 里调用 Checkpoint
import great_expectations as gx

# 获取数据上下文
context = gx.get_context()

# 运行 Checkpoint
checkpoint_result = context.run_checkpoint(checkpoint_name="multi_source_checkpoint")

# 打印结果
print(checkpoint_result)


方法 2：使用 Validation Operator 运行多个数据源
在 GX 1.3.6 版本，你可以用 run_validation_operator() 手动运行相同的 Expectation Suite 来检查多个数据源。
import great_expectations as gx
from great_expectations.core.batch import BatchRequest

# 获取数据上下文
context = gx.get_context()

# 定义多个数据源的 BatchRequest
batch_requests = [
    BatchRequest(
        datasource_name="postgres_datasource",
        data_connector_name="default_inferred_data_connector_name",
        data_asset_name="customers_table"
    ),
    BatchRequest(
        datasource_name="snowflake_datasource",
        data_connector_name="default_inferred_data_connector_name",
        data_asset_name="transactions_table"
    ),
    BatchRequest(
        datasource_name="s3_datasource",
        data_connector_name="default_inferred_data_connector_name",
        data_asset_name="logs_parquet"
    )
]

# 运行同一套 Expectation Suite
expectation_suite_name = "data_quality_suite"

for batch_request in batch_requests:
    results = context.run_validation_operator(
        "action_list_operator",
        assets_to_validate=[context.get_batch(batch_request, expectation_suite_name=expectation_suite_name)]
    )
    print(results)
方法 2 适用场景
	•	开发和测试环境
	•	手动执行多个数据源的验证
	•	不想使用 YAML 配置的情况


方法 3：动态创建多个 BatchRequest 并执行
如果你希望灵活地运行多个数据源，可以使用 BatchRequest 并批量执行验证。
batch_requests = [
    {
        "batch_request": {
            "datasource_name": "postgres_datasource",
            "data_connector_name": "default_inferred_data_connector_name",
            "data_asset_name": "customers_table"
        },
        "expectation_suite_name": "data_quality_suite"
    },
    {
        "batch_request": {
            "datasource_name": "snowflake_datasource",
            "data_connector_name": "default_inferred_data_connector_name",
            "data_asset_name": "transactions_table"
        },
        "expectation_suite_name": "data_quality_suite"
    },
    {
        "batch_request": {
            "datasource_name": "s3_datasource",
            "data_connector_name": "default_inferred_data_connector_name",
            "data_asset_name": "logs_parquet"
        },
        "expectation_suite_name": "data_quality_suite"
    }
]

for validation in batch_requests:
    batch_request = validation["batch_request"]
    expectation_suite = validation["expectation_suite_name"]
    context.run_validation_operator(
        "action_list_operator",
        assets_to_validate=[context.get_batch(batch_request, expectation_suite_name=expectation_suite)]
    )
方法 3 适用场景
	•	动态数据源
	•	手动控制不同的数据源
	•	需要灵活运行不同的批次
